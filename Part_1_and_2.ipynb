{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Harlinton Palacios Mosquera</center>\n",
    "# <center>161041033</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <CENTER>PART 1. Classifier Based on KNN using Euclidean distance.</CENTTER>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I import all the different libraries that I will use in the development of this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclideandis_distance(x,y):\n",
    "    dis=0\n",
    "    for i in range(0, len(x)):\n",
    "        dis += math.pow((x[i] - y[i]), 2)\n",
    "    return math.sqrt(dis)\n",
    "\n",
    "# manhattan distance\n",
    "def manhattandis_distance (x,y):\n",
    "    dis=0\n",
    "    for i in range(0, len(x)):\n",
    "        dis += abs(x[i] - y[i])\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    data_set = []\n",
    "    with open(file_path) as f:\n",
    "        for row in f:\n",
    "            row = row.replace(\"\\n\", \"\")\n",
    "            row = row.replace(\"\\t\", \" \")\n",
    "            tab = row.split(\" \")\n",
    "            tab = filter(None, tab)\n",
    "            data_set.append(map(float, tab))\n",
    "        np.random.shuffle(data_set)\n",
    "    return np.array(data_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I define the two equations I am going to use:\n",
    "\n",
    "<b>The Euclidean distance or Euclidean metric</b> is the \"ordinary\" straight-line distance between two points in Euclidean space. With this distance, Euclidean space becomes a metric space. The associated norm is called the Euclidean norm. Older literature refers to the metric as Pythagorean metric.\n",
    "\n",
    "<img src=\"http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sphilip/images/euclid_eqn.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "<b>Manhattan distance (plural Manhattan distances)</b> The distance between two points in a grid based on a strictly horizontal and/or vertical path (that is, along the grid lines), as opposed to the diagonal or \"as the crow flies\" distance.\n",
    "\n",
    "<img src=\"http://angiogenesis.dkfz.de/oncoexpress/software/cs_clust/dist_004.gif\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(Xtrain, Xtest, Ytrain, k, distanceMetric):\n",
    "    Ypred = []\n",
    "    for vect in Xtest:\n",
    "        distances = []\n",
    "        for x, y in zip(Xtrain, Ytrain):\n",
    "            distances.append((y, distanceMetric(x, vect)))\n",
    "        distances.sort(key=operator.itemgetter(1))\n",
    "        distances = distances[:k]\n",
    "        dictionary = {}\n",
    "        for tuple in distances:\n",
    "            #if not dictionary.has_key(tuple[0]):\n",
    "            if not (tuple[0]) in  dictionary:  \n",
    "                dictionary[tuple[0]] = 1\n",
    "            else:\n",
    "                dictionary[tuple[0]] += 1\n",
    "        Ypred.append(max(dictionary, key=dictionary.get))\n",
    "    return Ypred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    " \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I generate the function of the nearest neighbor, adding the necessary parameters for its execution, such as training data, test data of the equation and the value of k.\n",
    "\n",
    "<b>K nearest neighbors </b> is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970's as a non-parametric technique\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion(Ytest, Ypred, nb):\n",
    "    confmat = np.zeros((nb, nb))\n",
    "    for y, y_ in zip(Ytest, Ypred):\n",
    "        confmat[y-1][y_-1] += 1\n",
    "    return confmat\n",
    "\n",
    "\n",
    "def calculate_recall(confmat):\n",
    "    recall = 0\n",
    "    i = 0\n",
    "    for x in np.diagonal(confmat):\n",
    "        s = sum(np.transpose(confmat)[i])\n",
    "        if s == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        recall += x / s\n",
    "        i+=1\n",
    "    return recall / i\n",
    "\n",
    "def calculate_precision(confmat):\n",
    "    precision = 0\n",
    "    i = 0\n",
    "    for x in np.diagonal(confmat):\n",
    "        s = sum(confmat[i])\n",
    "        if s == 0:\n",
    "            i += 1\n",
    "            continue\n",
    "        precision += x / s\n",
    "        i+=1\n",
    "    return precision / i\n",
    "\n",
    "def calculate_acc(confmat):\n",
    "    return sum(confmat.diagonal()) / sum(sum(confmat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I generate all the functions that will help me with giving results, such as:\n",
    "\n",
    "• Recall function it takes the confusion matrix for calculating the recall measure. \n",
    "\n",
    "• Precision function: it takes the confusion matrix for calculating the precision measure.\n",
    "\n",
    "• Accuracy function: it takes the confusion matrix for calculating the accuracy measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY :  0.952991452991\n",
      "PRECISION :  0.949118589744\n",
      "RECALL :  0.949164377289\n",
      "Time: 0.02001190185546875 secs\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "data_leaf='leaf.dat'\n",
    "data = read_data(data_leaf)\n",
    "splits = 4\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "class_names = iris.target_names\n",
    "k = 5\n",
    "accuracy = 0\n",
    "recall = 0\n",
    "precision = 0\n",
    "ella= 4\n",
    "kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    Xtrain = X[train_index]\n",
    "    Xtest = X[test_index]\n",
    "    Ytrain = y[train_index]\n",
    "    Ytest = y[test_index]\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    Ypred = k_nearest_neighbors(Xtrain, Xtest, Ytrain, k, manhattandis_distance)\n",
    "    \n",
    "    confmat = confusion(Ytest, Ypred, max(Ytrain))\n",
    "    accuracy += calculate_acc(confmat)\n",
    "    recall += calculate_recall(confmat)\n",
    "    precision += calculate_precision(confmat)\n",
    "\n",
    "print (\"ACCURACY : \", (accuracy) / (ella))\n",
    "print (\"PRECISION : \", precision / (ella))\n",
    "print (\"RECALL : \", recall /(ella))\n",
    "print(\"Time: %s secs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leaf.dat\n",
    "ACCURACY  = 0.129411764706\n",
    "\n",
    "PRECISION = 0.113078703704\n",
    "\n",
    "RECALL = 0.11539376904\n",
    "\n",
    "Time 0.259999990463 secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12  0  0]\n",
      " [ 0 11  1]\n",
      " [ 0  1 11]]\n",
      "Normalized confusion matrix\n",
      "[[ 1.    0.    0.  ]\n",
      " [ 0.    0.92  0.08]\n",
      " [ 0.    0.08  0.92]]\n"
     ]
    }
   ],
   "source": [
    "def confusionMatrices():\n",
    "    cnf_matrix = confusion_matrix(Ytest, Ypred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "    \n",
    "confusionMatrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one of the most important funsion because it generates the results of ACC, PRECISION, RECALL AND EXECUTION TIME also this section I declare some vital variables for the execution of algorithm such as:\n",
    "(K) related to Nearest Neighbor\n",
    "(iris) saves the data from the iris database\n",
    "(splits) Number of folds\n",
    "X, and I assign the data and the targets of the iris data\n",
    "Accuracy, recall and precision which keep the data corresponding to each variable.\n",
    "\n",
    "\n",
    "Also in this session I download the iris data base from the sklearn.datasets library, which is a simpler way to get some databases without downloading  files and then read it also in this section is the division of data, for this I used k-fold cross validation.\n",
    "\n",
    "In KFolds, each test set should not overlap, even with shuffle. With KFolds and shuffle, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest. In ShuffleSplit, the data is shuffled every time, and then split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Part 2: classifier based on KNN using Manhattan distance.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY :  0.952457264957\n",
      "PRECISION :  0.943509615385\n",
      "RECALL :  0.951163836164\n",
      "Time: 0.024018526077270508 secs\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "recall = 0\n",
    "precision = 0\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "\n",
    "for train_index, test_index in kfold.split(X, y):\n",
    "    Xtrain = X[train_index]\n",
    "    Xtest = X[test_index]\n",
    "    Ytrain = y[train_index]\n",
    "    Ytest = y[test_index]\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    Ypred = k_nearest_neighbors(Xtrain, Xtest, Ytrain, k, euclideandis_distance)\n",
    "    \n",
    "    \n",
    "    confmat = confusion(Ytest, Ypred, max(Ytrain))\n",
    "    accuracy += calculate_acc(confmat)\n",
    "    recall += calculate_recall(confmat)\n",
    "    precision += calculate_precision(confmat)\n",
    "\n",
    "print (\"ACCURACY : \", (accuracy) / (ella))\n",
    "print (\"PRECISION : \", precision / (ella))\n",
    "print (\"RECALL : \", recall /(ella))\n",
    "print(\"Time: %s secs\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# leaf.dat\n",
    "ACCURACY:  0.254901960784\n",
    "\n",
    "PRECISION:  0.252698412698\n",
    "\n",
    "RECALL:     0.354533497903\n",
    "\n",
    "Time:  0300002098083\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[12  0  0]\n",
      " [ 0 10  2]\n",
      " [ 0  1 11]]\n",
      "Normalized confusion matrix\n",
      "[[ 1.    0.    0.  ]\n",
      " [ 0.    0.83  0.17]\n",
      " [ 0.    0.08  0.92]]\n"
     ]
    }
   ],
   "source": [
    "def confusionMatrices():\n",
    "    cnf_matrix = confusion_matrix(Ytest, Ypred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "    \n",
    "confusionMatrices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In relation to paragraphs 1 and 2 in which we used two different distance metrics (Euclidean distance and Manhattan distance) in conjunction with Iris data set, we can observe that for this set of data we obtained very similar results using Euclidean distance all the values of Accuracy and accuracy are very similar to the Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       0.91      0.83      0.87        12\n",
      "          2       0.85      0.92      0.88        12\n",
      "\n",
      "avg / total       0.92      0.92      0.92        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(Ytest, Ypred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
